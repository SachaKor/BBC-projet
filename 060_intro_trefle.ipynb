{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 060 - Features and model selection - Exploration of TREFLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab developed by: Diogo  Leite - 03.2019<br>\n",
    "Trefle algorithm: Gary Marigliano. (Based on the PhD thesis of Carlos Pe√±a https://infoscience.epfl.ch/record/33110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "<br>\n",
    "In this notebook, we use the Breast Cancer Wisconsin Diagnostic (BCWD) dataSet. You can find more details here: \n",
    "\n",
    "[UCI](http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic%29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO in this notebook\n",
    "You should provide your answers to the questions of this notebook in a report (Note that a short and concise report with the essential information is **much** better than a long one that tells nothing...). Just indicate clearly the number of the question and give the respective answer. If you need plots to confirm your observations, include them also. At the end, send the notebook in annex to your report.\n",
    "<br>\n",
    "Sometimes you will need to select (decide on) some values as a way to perform filters that reduce the number of models (and save the bests).\n",
    "<br>\n",
    "<b>Some experiences take time (up to several hours), consider that in order to don't do your lab at the last minute (all the experiments are potentially different as TREFLE isn't a deterministic algorithm).\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to submit your lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export all the notebooks in HTML format (in the case your lab could not be reproduced for any reason) + zip your whole lab folder without the dataset(s). If your lab requires additional dependencies, please add a INSTRUCTIONS.md file in your folder with the instructions to install them. Don't forget to add the additional dependencies at the end of your requirements.txt file (or do a pip freeze > requirements_personal.txt command).\n",
    "<br>\n",
    "You must write a short report (as mentioned above) with three sections (one for each dataset) and respond to all the questions in each section. Include the report (<b>PDF and only PDF</b>) on the zip. The organization of the report must follow the structure below:\n",
    "<ul>\n",
    "    <li>Dataset BCWD</li>\n",
    "    <ul>\n",
    "        <li>Question 1 </li>\n",
    "        <li>Question ... </li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the values (parameters of the algorithm) provided in this notebook are given only as example and may not be adequate for your lab. You will need to make decisions on these values, and sometimes justify them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preparatory stage\n",
    "\n",
    "## Set up the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from itertools import tee\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from trefle.fitness_functions.output_thresholder import round_to_cls\n",
    "from trefle.trefle_classifier import TrefleClassifier\n",
    "\n",
    "from trefle_engine import TrefleFIS\n",
    "\n",
    "\n",
    "\n",
    "import libraries.measures_calculation\n",
    "import libraries.trefle_project\n",
    "import libraries.interpretability_methods\n",
    "import libraries.interpretability_plots\n",
    "import libraries.results_plot\n",
    "from libraries.model_var import ModelVar\n",
    "from libraries.model_train_cv import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset\n",
    "<br>\n",
    "The first step of the ML process is to split our dataset into training and test parts (subsets). <br> \n",
    "<ul>\n",
    "    <li>You must indicate the path of your original dataset</li>\n",
    "    <li>You must indicate the path where you want to save the training part</li>\n",
    "    <li>You must indicate the path where you want to save the test part</li>\n",
    "</ul>\n",
    "<br>When a plot is \"open\" you need to \"shut it down\" in order to plot the others (button on the upper corner right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Dataset\n",
    "#Indicate the path of the original DS HERE:\n",
    "#--------------------\n",
    "csv_path_file_name = './datasets/WDBC/data_WDBC.csv'\n",
    "#--------------------\n",
    "\n",
    "data_load = pd.read_csv(csv_path_file_name, sep = ',')\n",
    "\n",
    "#Before continue, please remove the variables that you don't want to use along this lab.\n",
    "\n",
    "#Use to drop columns\n",
    "#data_load.drop(['p1', 'p13'], axis=1)\n",
    "\n",
    "X = data_load.iloc[:, 1:-1]\n",
    "y = data_load.iloc[:,-1]\n",
    "\n",
    "#Split it into train test DS\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, stratify=y, random_state=42, test_size=0.33)\n",
    "\n",
    "plt.hist(y_train, bins='auto', label='Train')\n",
    "\n",
    "plt.hist(y_test, bins='auto', label='Test')\n",
    "plt.title(\"Train test Split\")\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Quantity')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Save separetly in training and test\n",
    "#It is important to save the training and test sets (we will use the test in the second part)\n",
    "y_train_modify = np.reshape(y_train, (-1, 1))\n",
    "train_dataset = np.append(X_train, y_train_modify, axis=1)\n",
    "\n",
    "y_test_modify = np.reshape(y_test, (-1, 1))\n",
    "test_dataset = np.append(X_test, y_test_modify, axis=1)\n",
    "\n",
    "#This indicates to numpy how to format the output (you can create a function for a larger number of variables...)\n",
    "#format_values = '%1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %i'\n",
    "\n",
    "\n",
    "#Indicate the path where you want to save the training and test part DS HERE:\n",
    "#--------------------\n",
    "path_train_csv = './datasets/WDBC/data_WDBC_train_v2.csv'\n",
    "path_test_csv = './datasets/WDBC/data_WDBC_test_v2.csv'\n",
    "#--------------------\n",
    "np.savetxt(path_train_csv, train_dataset, delimiter=\",\")\n",
    "np.savetxt(path_test_csv, test_dataset, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color:red;color:white\">Question 1</b>:  Comment the plot above (include it into your report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trefle Classifier\n",
    "<br> In the code below you have a description of the (fuzzy logic-based) classifier that we use along this labo, the theory is provided in the slides of the cours. <br>\n",
    "Don't forget to change, if necessary, the number of generations (iterations) of your algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize our classsifier TREFLE\n",
    "clf = TrefleClassifier(\n",
    "    n_rules=4,\n",
    "    n_classes_per_cons=[2],  # there is only 1 consequent with 2 classes\n",
    "    n_labels_per_mf=3,  # use 3 labels LOW, MEDIUM, HIGH\n",
    "    default_cons=[0],  # default rule yield the class 0\n",
    "    n_max_vars_per_rule=3,  # WBCD dataset has 30 variables, here we force\n",
    "    # to use a maximum of 3 variables per rule\n",
    "    # to have a better interpretability\n",
    "    # In total we can have up to 3*4=12 different variables\n",
    "    # for a fuzzy system\n",
    "    \n",
    "    #Change here the number of generations (if necessary)\n",
    "    n_generations=250,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and predicting with Trefle\n",
    "<br> Below you have a simple example of how to:<br>\n",
    "<ul>\n",
    "    <li>train a model and make a prediction with it</li>\n",
    "    <li>save the model in a file</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a train\n",
    "y_sklearn = np.reshape(y_train, (-1, 1))\n",
    "\n",
    "clf.fit(X_train, y_sklearn)\n",
    "# Make predictions\n",
    "y_pred = clf.predict_classes(X_test)\n",
    "\n",
    "clf.print_best_fuzzy_system()\n",
    "\n",
    "# Evaluate accuracy\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(\"Score on test set: {:.3f}\".format(score))\n",
    "\n",
    "tff = clf.get_best_fuzzy_system_as_tff()\n",
    "\n",
    "# Export: save the fuzzy model to disk\n",
    "with open(\"my_saved_model_trefle.tff\", mode=\"w\") as f:\n",
    "    f.write(tff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Launch the Trefle experiments (or modeling runs)\n",
    "In this labo we perform k-fold cross-validation, so you must indicate how many folds do you want (by default 10). If you don't understand this concept, investigate it and/or discuss it with your class mates, the TA or the professor.\n",
    "\n",
    "We could perform an exhaustive search for many parameters of the algorithm but, for this labo we will only search for parameters related with the size (complexity) of the model: i.e., number of rules and variables per rule.\n",
    "\n",
    "**Note 1:** You must indicate the path where you want to save all the models obtained. <br>\n",
    "\n",
    "**Note 2:** You must choose and justify the range of values you will explore for:\n",
    "<ul>\n",
    "    <li>the different weights (importance) for the three criteria: sensitivity, specificity, and RMSE</li>\n",
    "    <li>the number of rules</li>\n",
    "    <li>the maximum number of variables per rule </li>\n",
    "</ul>\n",
    "\n",
    "The code must be adapted according to your choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Search for fitness function weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the lab focuses on balancing the three criteria that we want to use as performance metrics. In our case, we will concentrate on maximising the sensitivity and the specificity (related with diagnostic performance) and minimizing the RMSE, related with numeric precision. To do so, we will search for a combination (weights) of these 3 criteria  that facilitates the search to the algorithm. You can look at the slides for more details.\n",
    "\n",
    "### First: equilibrating weights for sensitivity and specificity\n",
    "The first exploration, performed below, looks for an adequate combination of weights for sensitivity and sensitivity by means of the balancing parameter alpha.\n",
    "<br>\n",
    "IMPORTANT: analyze the comments in the code and perform the modifications that are necessary for the proposed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "global weigh_senSpe\n",
    "\n",
    "\n",
    "##############fitness function (No change required)\n",
    "def fit (y_true, y_pred):\n",
    "    global weigh_senSpe\n",
    "    \n",
    "    y_pred_bin = round_to_cls(y_pred, n_classes=2)\n",
    "    tn, fp, fn, tp = libraries.trefle_project.getConfusionMatrixValues(y_true, y_pred_bin)\n",
    "    \n",
    "    \n",
    "    sensitivity = libraries.measures_calculation.calculateSensitivity(tn, fp, fn, tp)\n",
    "    specificity = libraries.measures_calculation.calculateSpecificity(tn, fp, fn, tp)\n",
    "    #rmse = mean_squared_error(y_true, y_pred)\n",
    "    score = weigh_senSpe * sensitivity + (1.0 - weigh_senSpe) * specificity\n",
    "    return score\n",
    "\n",
    "clf.fitness_function=fit\n",
    "###############\n",
    "\n",
    "\n",
    "#Perform Cross-validation\n",
    "#Change here the number of folds (if necessary)\n",
    "k_fold_number = 10\n",
    "cv_kf = KFold(n_splits=k_fold_number, random_state=42, shuffle=True)\n",
    "array_index_train_test = cv_kf.split(X_train)\n",
    "array_index_train_test, array_index_train_test_copy = tee(array_index_train_test)\n",
    "\n",
    "\n",
    "#--------------------\n",
    "#Path where you want to save yours models (you need to create the directory befor start the algorithm)\n",
    "path_save_results_directory = 'experiences/sen_spe/'\n",
    "#file nam that will contain the results for each model create (so fo each fold)\n",
    "file_results_dv = 'values_sen_spe_weight.csv'\n",
    "#Name of the experience, this name will appear on the models files\n",
    "experience_value_name = 'exps_lab_lfa_senSpe_2'\n",
    "#--------------------\n",
    "\n",
    "model_train_obj = ModelTrain(array_index_train_test = array_index_train_test,\n",
    "                             X_train = X_train,\n",
    "                             y_train = y_train, \n",
    "                             number_rule = 0, var_per_rule = 0, \n",
    "                             classifier_trefle = clf, \n",
    "                             path_save_results = path_save_results_directory,\n",
    "                            path_save_results_values=file_results_dv,\n",
    "                            experience_name = experience_value_name)\n",
    "\n",
    "\n",
    "#Here we can choose which values for the number of rules and maximum variables per \n",
    "#rule we want to test along our experience ('here you need to change and explain your choice, on the report')\n",
    "vec_weight = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "#For the moment we use these values for the number of rules and vars per rule\n",
    "number_rule = 5\n",
    "var_per_rule = 5\n",
    "\n",
    "for weight_actual in vec_weight:\n",
    "    model_train_obj.number_rule = number_rule\n",
    "    model_train_obj.var_per_rule = var_per_rule\n",
    "    model_train_obj.weight_actual = weight_actual\n",
    "    weigh_senSpe = weight_actual\n",
    "    model_train_obj.execute_cv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List result files\n",
    "When the modeling experiments are performed, we calculate the average of the scores for each configuration according to the number of folds for several metrics/measurements (accuracy, f1-score, sensitivity, and specificity). \n",
    "<br>Don't forget to change the file where you have the results for the models.\n",
    "<br>For curiousity sake, you may implement other metrics in the \"measures_calculation\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2896732e155a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#read all csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#--------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataframe_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'values_sen_spe_weight.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#--------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#Plot sen spe resuts\n",
    "#read all csv\n",
    "#--------------------\n",
    "dataframe_results = pd.read_csv('values_sen_spe_weight.csv')\n",
    "#--------------------\n",
    "\n",
    "\n",
    "dataframe_results.columns = ['N rule', 'N var per rule','Weight', 'CV number', 'tn', 'fp', 'fn', 'tp', 'file_name']\n",
    "#display(dataframe_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'libraries' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2a1cdb4521e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Plot all values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#don't forget to turn off the others plotss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvec_values_sen_spe_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibraries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpretability_methods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetSenSpeValuesByScoresWeigh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'libraries' is not defined"
     ]
    }
   ],
   "source": [
    "#Plot all values\n",
    "#don't forget to turn off the others plotss\n",
    "vec_values_sen_spe_models = libraries.interpretability_methods.getSenSpeValuesByScoresWeigh(dataframe_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_results = libraries.interpretability_methods.getMeanSenSpeByWeight(vec_values_sen_spe_models)\n",
    "display(dataframe_results)\n",
    "\n",
    "#dataframe_results['product'] = dataframe_results.Sensitivity * dataframe_results.Specificity\n",
    "\n",
    "\n",
    "libraries.interpretability_plots.plotSenSpeWeigh(dataframe_results, 'Sen/Spe - Weigh')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color:red;color:white\">Question 2</b>: Explain what is the meaning/role of the alpha value? Why we \"play\" with it? How is it related with the weights given to sebnsitivity and specificity? What would imply a high weight for sensitivity, respectively specificity?\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color:red;color:white\">Question 3 </b>: Decide on an alpha value to be used to define the (fitness) weights for sensitivity and specificity. Explain your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second: finding the right contribution of the RMSE\n",
    "\n",
    "Now that you have selected a balance between sensitivity and specificity, you can search for an adequate RMSE weight in the same way.\n",
    "<br>\n",
    "**Important:** Don't forget to change the values of the weights for sensitivity and specificity according to your previous choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "global weigh_RMSE\n",
    "\n",
    "##############fitness function\n",
    "def fit (y_true, y_pred):\n",
    "    global weigh_RMSE\n",
    "    \n",
    "    y_pred_bin = round_to_cls(y_pred, n_classes=2)\n",
    "    tn, fp, fn, tp = libraries.trefle_project.getConfusionMatrixValues(y_true, y_pred_bin)\n",
    "    \n",
    "    \n",
    "    sensitivity = libraries.measures_calculation.calculateSensitivity(tn, fp, fn, tp)\n",
    "    specificity = libraries.measures_calculation.calculateSpecificity(tn, fp, fn, tp)\n",
    "    rmse = mean_squared_error(y_true, y_pred)\n",
    "#Change here the values of the weigh choosed    \n",
    "    weight_sen = 0.6 * (1.0 - weigh_RMSE)\n",
    "    weight_spe = 0.4 * (1.0 - weigh_RMSE)\n",
    "    \n",
    "    #score = weight_sen * sensitivity + weight_spe * specificity + weigh_RMSE * rmse\n",
    "    score = weight_sen * sensitivity + weight_spe * specificity + weigh_RMSE * math.pow(2, -rmse)\n",
    "    return score\n",
    "\n",
    "clf.fitness_function=fit\n",
    "\n",
    "#--------------------\n",
    "#Path where you want to save yours models (you need to create the directory befor start the algorithm)\n",
    "path_save_results_directory = 'experiences/rmse_v2/'\n",
    "#file nam that will contain the results for each model create (so fo each fold)\n",
    "file_results_dv = 'values_rmse_weight_v2.csv'\n",
    "#Name of the experience, this name will appear on the models files\n",
    "experience_value_name = 'exps_lab_lfa_rmse_v2_rmse'\n",
    "#--------------------\n",
    "\n",
    "model_train_obj = ModelTrain(array_index_train_test = array_index_train_test,\n",
    "                             X_train = X_train,\n",
    "                             y_train = y_train, \n",
    "                             number_rule = 0, var_per_rule = 0, \n",
    "                             classifier_trefle = clf, \n",
    "                             path_save_results = path_save_results_directory,\n",
    "                            path_save_results_values=file_results_dv,\n",
    "                            experience_name = experience_value_name)\n",
    "\n",
    "\n",
    "#Here you can define wich values/ranges you explore for the RMSE weights \n",
    "#('here you need to change and explain your choice, on the report')\n",
    "#--------------------\n",
    "vec_weight = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "number_rule = 5\n",
    "var_per_rule = 5\n",
    "#--------------------\n",
    "\n",
    "for weight_actual in vec_weight:\n",
    "    model_train_obj.number_rule = number_rule\n",
    "    model_train_obj.var_per_rule = var_per_rule\n",
    "    model_train_obj.weight_actual = weight_actual\n",
    "    \n",
    "    weigh_RMSE = weight_actual\n",
    "    model_train_obj.execute_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot sen spe resuts\n",
    "#read all csv\n",
    "#--------------------\n",
    "dataframe_results = pd.read_csv('values_rmse_weight_v2.csv')\n",
    "#--------------------\n",
    "\n",
    "#dataframe_results_c = pd.read_csv('values_w.csv')\n",
    "dataframe_results.columns = ['N rule', 'N var per rule','Weight', 'CV number', 'tn', 'fp', 'fn', 'tp', 'file_name']\n",
    "#display(dataframe_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "#Plot all values\n",
    "#don't forget to turn off the others plotss\n",
    "vec_values_sen_spe_models_w_rmse = libraries.interpretability_methods.getSenSpeValuesByScoresWeigh(dataframe_results)\n",
    "#print(vec_values_sen_spe_models_w_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "\n",
    "\n",
    "dataframe_results = libraries.interpretability_methods.getMeanSenSpeByWeight(vec_values_sen_spe_models_w_rmse)\n",
    "display(dataframe_results)\n",
    "\n",
    "#dataframe_results['product'] = dataframe_results.Sensitivity * dataframe_results.Specificity\n",
    "\n",
    "\n",
    "libraries.interpretability_plots.plotSenSpeWeigh(dataframe_results, 'RMSE - Weigh')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color:red;color:white\">Question 4 </b>: Analyze the graphic above and decide on a weight for the RMSE contribution to the fitness funnction. Justify your choice. What are your final values for the three weights? How do you interpret them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Model-parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the fitness function have been defined, we may search values for other parameters of the algorithm. In this part we will focus on the size (complexity) of the model, represented by the number of rules and the number of variables per rule.\n",
    "\n",
    "<b style=\"background-color:red;color:white\">Question 5</b>: Explain what are the implications of these two parameters (i.e., number of rules and number of variables per rule) on the models, in terms of both performance and interpretability.\n",
    "<br>\n",
    "<b style=\"background-color:red;color:white\">Question 6</b>: If you have setted your algorithm up to use 6 rules and 5 variables per rule on a dataset composeed of 100 features, how many features could be used at most by a model?\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Important:** Before continuing, don't forget to set the right weights for sensitivity, specificity, and RMSE!\n",
    "\n",
    "### Coarse estimation of the model size\n",
    "Not knowing the complexity of the required models, we must first roughly estimate them. This is done by exploring a relatively large range of model sizes. Performing a grid search (i.e., exploring both parameters simultaneously) would be the best approach, but that may be extremely costly and time consuming. Instead, we will explore one of the parameters, the number of rules. \n",
    "\n",
    "** Note:** Before performing the experiments, don't forget to set the values for the <b>rules_number_vec</b>. They represent the number of rules, pay attention to the size of the model. Don't change the value of 'var_per_rule_fix'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color:red;color:white\">Question 7</b>: In your opinion, why did we decide to first explore the number of rules instead of the number of variables per rule?\n",
    "<br>\n",
    "<b style=\"background-color:red;color:white\">Question 8</b>: Which values have you decided to test at this stage? Why this range?\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "##############fitness function\n",
    "def fit (y_true, y_pred):\n",
    "    \n",
    "    y_pred_bin = round_to_cls(y_pred, n_classes=2)\n",
    "    tn, fp, fn, tp = libraries.trefle_project.getConfusionMatrixValues(y_true, y_pred_bin)\n",
    "    \n",
    "    \n",
    "    sensitivity = libraries.measures_calculation.calculateSensitivity(tn, fp, fn, tp)\n",
    "    specificity = libraries.measures_calculation.calculateSpecificity(tn, fp, fn, tp)\n",
    "    rmse = mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "#--------------------\n",
    "    weigh_RMSE = 0.1\n",
    "    weight_sen = 0.6 * (1.0 - weigh_RMSE)\n",
    "    weight_spe = 0.4 * (1.0 - weigh_RMSE)\n",
    "#--------------------\n",
    "    score = weight_sen * sensitivity + weight_spe * specificity + weigh_RMSE * math.pow(2, -rmse)\n",
    "    return score\n",
    "\n",
    "clf.fitness_function=fit\n",
    "###############\n",
    "\n",
    "#Path where you want to save yours models (you need to create the directory befor start the algorithm)\n",
    "path_save_results_directory = 'experiences/n_rules/'\n",
    "#file nam that will contain the results for each model create (so fo each fold)\n",
    "file_results_dv = 'values_number_of_rules.csv'\n",
    "#Name of the experience, this name will appear on the models files\n",
    "experience_value_name = 'exps_lab_lfa_number_of_rules'\n",
    "\n",
    "model_train_obj = ModelTrain(array_index_train_test = array_index_train_test,\n",
    "                             X_train = X_train,\n",
    "                             y_train = y_train, \n",
    "                             number_rule = 0, var_per_rule = 0, \n",
    "                             classifier_trefle = clf, \n",
    "                             path_save_results = path_save_results_directory,\n",
    "                            path_save_results_values=file_results_dv,\n",
    "                            experience_name = experience_value_name)\n",
    "\n",
    "\n",
    "#Here we can choose wich values for the number of rules and maximum variable per \n",
    "#rule we want to test along our experience ('here you need to change and explain your choice, on the repport')\n",
    "#--------------------\n",
    "rules_number_vec = [3, 5, 8, 10]\n",
    "var_per_rule_fix = 5\n",
    "#--------------------\n",
    "\n",
    "for qty_of_rule in rules_number_vec:\n",
    "    model_train_obj.number_rule = qty_of_rule\n",
    "    model_train_obj.var_per_rule = var_per_rule_fix\n",
    "    model_train_obj.execute_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "#Plot sen spe resuts\n",
    "#read all csv\n",
    "#--------------------\n",
    "dataframe_results = pd.read_csv('values_number_of_rules.csv')\n",
    "#--------------------\n",
    "\n",
    "#dataframe_results_c = pd.read_csv('values_w.csv')\n",
    "dataframe_results.head()\n",
    "\n",
    "param_a_designation = 'nb of rules'\n",
    "param_b_designation = 'nb of var per rule'\n",
    "\n",
    "vec_measures = ['acc', 'f1', 'sen', 'spe']\n",
    "\n",
    "\n",
    "\n",
    "test_data = dataframe_results.iloc[:,0:2]\n",
    "\n",
    "\n",
    "data_frame_treated = libraries.trefle_project.treatmentResultsValues(dataframe_results, param_a_designation, param_b_designation, vec_measures)\n",
    "data_frame_treated.columns = ['N rule', 'N var per rule', 'acc', 'f1', 'sen', 'spe']\n",
    "display(data_frame_treated)\n",
    "\n",
    "\n",
    "\n",
    "libraries.interpretability_plots.plotSenSpeNRules(data_frame_treated, 'N rules')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finer search of parameters\n",
    "\n",
    "The next step will be to perform a grid search for both parameters on a narrow range of values. For this, we need to define ranges for them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color:red;color:white\">Question 9</b>: On the base of the graphic above, select a narrower range for the number of rules to be explored in the next step. Justify your choice.\n",
    "<br>\n",
    "<b style=\"background-color:red;color:white\">Question 10</b>: Then, define a range of values for the number of variables per rule. How did you decide on them? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Var per rule and number of rules\n",
    "\n",
    "\n",
    "#--------------------\n",
    "#Path where you want to save yours models (you need to create the directory befor start the algorithm)\n",
    "path_save_results_directory = 'experiences/n_rules_nvar/'\n",
    "#file nam that will contain the results for each model create (so fo each fold)\n",
    "file_results_dv = 'values_number_of_rules_nvar.csv'\n",
    "#Name of the experience, this name will appear on the models files\n",
    "experience_value_name = 'exps_lab_lfa_number_of_rules_var'\n",
    "#--------------------\n",
    "\n",
    "\n",
    "model_train_obj = ModelTrain(array_index_train_test = array_index_train_test,\n",
    "                             X_train = X_train,\n",
    "                             y_train = y_train, \n",
    "                             number_rule = 0, var_per_rule = 0, \n",
    "                             classifier_trefle = clf, \n",
    "                             path_save_results = path_save_results_directory,\n",
    "                            path_save_results_values=file_results_dv,\n",
    "                            experience_name = experience_value_name)\n",
    "\n",
    "\n",
    "\n",
    "#Here we can choose wich values for the number of rules and maximum variable per \n",
    "#rule we want to test along our experience ('here you need to change and explain your choice, on the repport')\n",
    "#--------------------\n",
    "rules_number_vec = [7,8,9]\n",
    "var_per_rule_vec = [2,3,5,7]\n",
    "#--------------------\n",
    "\n",
    "for variation_a in rules_number_vec:\n",
    "    for variation_b in var_per_rule_vec:\n",
    "        model_train_obj.number_rule = variation_a\n",
    "        model_train_obj.var_per_rule = variation_b\n",
    "        model_train_obj.execute_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load models\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "\n",
    "#play with the results of the differents executions\n",
    "data = pd.read_csv(\"values_number_of_rules_nvar.csv\") \n",
    "# Preview the first 5 lines of the loaded data \n",
    "data.head()\n",
    "\n",
    "param_a_designation = 'nb of rules'\n",
    "param_b_designation = 'nb of var per rule'\n",
    "\n",
    "vec_measures = ['acc', 'f1', 'sen', 'spe']\n",
    "\n",
    "\n",
    "\n",
    "test_data = data.iloc[:,0:2]\n",
    "\n",
    "\n",
    "data_frame_treated = libraries.trefle_project.treatmentResultsValues(data, param_a_designation, param_b_designation, vec_measures)\n",
    "data_frame_treated.columns = ['N rule', 'N var per rule', 'acc', 'f1', 'sen', 'spe']\n",
    "display(data_frame_treated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize with 3D graphs\n",
    "Below you may visualize the performance of your models according to the explored paramaters: number of rules and number of variables per rule. You may change the code so as to make plots for different metrics (Acc, F1, Sen and Spe). You could also add new/different metrics by creating the corresponding method in the 'measures_calculation' class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot 3D\n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "\n",
    "X = data_frame_treated['N rule']\n",
    "Y = data_frame_treated['N var per rule']\n",
    "Z = data_frame_treated['acc']\n",
    "\n",
    "y_axis_values = range(math.floor(min(Y)), math.ceil(max(Y))+1)\n",
    "x_axis_values = range(math.floor(min(X)), math.ceil(max(X))+1)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "surf = ax.plot_trisurf(X, Y, Z,  cmap=cm.YlGnBu, linewidth=0, antialiased=False)\n",
    "\n",
    "#ax.set_zlim(-1.01, 1.01)\n",
    "ax.set_xticks(x_axis_values, minor=False)\n",
    "ax.set_yticks(y_axis_values, minor=False)\n",
    "\n",
    "ax.set_xlabel('$Number of rules$')\n",
    "ax.set_ylabel('$Number of var per rule$')\n",
    "\n",
    "\n",
    "ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "\n",
    "\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "plt.title('Sensitivity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color:red;color:white\">Question 11</b> In your opinion, which values/ranges of both parameters: number of rules and vars per rule, should you choose to obtain the best models? (comment briefly on the plot and include it into to report)\n",
    "\n",
    "Don't forget to change those values below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional refinement of the parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we observed the plot we can refine the search for parameter values. As for the previous experiment it is necessary to:\n",
    "<ul>\n",
    "    <li>define new values/ranges for the number of rules </li>\n",
    "    <li>define new values/ranges number of variable per rule </li>\n",
    "    <li>change the path name where you want to save the new models </li>\n",
    "    <li>change the name of the file that will contain the number of experiments</li>\n",
    "     \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Var per rule and number of rules\n",
    "\n",
    "\n",
    "#Change the path directory where you want to save the new results\n",
    "#--------------------\n",
    "#Path where you want to save yours models (you need to create the directory befor start the algorithm)\n",
    "path_save_results_directory = 'experiences/n_rules_nvar_tuning/'\n",
    "#file nam that will contain the results for each model create (so fo each fold)\n",
    "file_results_dv = 'values_number_of_rules_nvar_tuning.csv'\n",
    "#Name of the experience, this name will appear on the models files\n",
    "experience_value_name = 'exps_lab_lfa_number_of_rules_var_tuning'\n",
    "#--------------------\n",
    "\n",
    "\n",
    "model_train_obj = ModelTrain(array_index_train_test = array_index_train_test,\n",
    "                             X_train = X_train,\n",
    "                             y_train = y_train, \n",
    "                             number_rule = 0, var_per_rule = 0, \n",
    "                             classifier_trefle = clf, \n",
    "                             path_save_results = path_save_results_directory,\n",
    "                            path_save_results_values=file_results_dv,\n",
    "                            experience_name = experience_value_name)\n",
    "\n",
    "\n",
    "#Here we can choose wich values for the number of rules and maximum variable per \n",
    "#rule we want to test along our experience \n",
    "#('here you need to change and explain your choice, on the repport')\n",
    "#--------------------\n",
    "rules_number_vec = [6,7]\n",
    "var_per_rule_vec = [3,4]\n",
    "#--------------------\n",
    "\n",
    "for variation_a in rules_number_vec:\n",
    "    for variation_b in var_per_rule_vec:\n",
    "        model_train_obj.number_rule = variation_a\n",
    "        model_train_obj.var_per_rule = variation_b\n",
    "        model_train_obj.execute_cv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidating the results\n",
    "Now, put all yours models in the same directory (copy/past) and add all the csv results to the dataframe in order to analyse the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter sen/spe\n",
    "#read all csv\n",
    "dataframe_results = pd.read_csv('values_number_of_rules.csv')\n",
    "dataframe_results_b = pd.read_csv('values_number_of_rules_nvar.csv')\n",
    "dataframe_results_c = pd.read_csv('values_number_of_rules_nvar_tuning.csv')\n",
    "dataframe_results_d = pd.read_csv('values_rmse_weight.csv')\n",
    "dataframe_results_e = pd.read_csv('values_sen_spe_weight.csv')\n",
    "\n",
    "dataframe_results_all = dataframe_results_b.append(dataframe_results)\n",
    "dataframe_results_all = dataframe_results_all.append(dataframe_results_c)\n",
    "dataframe_results_all = dataframe_results_all.append(dataframe_results_d)\n",
    "dataframe_results_all = dataframe_results_all.append(dataframe_results_e)\n",
    "\n",
    "#dataframe_results_c = pd.read_csv('values_w.csv')\n",
    "dataframe_results_all.columns = ['N rule', 'N var per rule','weight','CV number', 'tn', 'fp', 'fn', 'tp', 'file_name']\n",
    "dataframe_results_all = dataframe_results_all.reset_index(drop=True)\n",
    "#display(dataframe_results_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have tested all the configurations, we have obtained a **large** number of models exhibiting diverse performance figures. At the end of a modeling process, the goal is to obtain one, or a few, models that would be deployed and used for new predictions. A selection process is thus necessary.\n",
    "\n",
    "A first selection is performed by applying a filter based on the diagnostic performance, thus reducing the number of models. Below you can see a scatter plot of all the models you obtained according to their sensitivity and specificity (as obtained on the validation subsets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot sent spe all\n",
    "\n",
    "#Plot all values\n",
    "#don't forget to turn off the others plotss\n",
    "vec_values_sen_spe_models = libraries.interpretability_methods.getSenSpeValuesByScores(dataframe_results_all)\n",
    "#vec_values_sen_spe_models = libraries.interpretability_methods.getSenSpeValuesByScores(data_frame_treated)\n",
    "\n",
    "plt.scatter(vec_values_sen_spe_models['Sensitivity'],vec_values_sen_spe_models['Specificity'],s=10, marker='o')\n",
    "\n",
    "plt.title('Threshold sen/spe')\n",
    "plt.xlabel('Sensitivity')\n",
    "plt.ylabel('Specificity')\n",
    "plt.savefig('ScatterPlot.png')\n",
    "\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.show()\n",
    "print('You have {0} models'.format(len(vec_values_sen_spe_models)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First selection filter: based on sen/spe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having analysed the above performance overview of your models, you can apply a filter based on  sensitivity and specificity. In this way, only those models exhibiting better performance than some specified threshold will be selected for the next step.\n",
    "The plot below shows the effect of the combined thresholds on the number of models remaining after the filter is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot sen_spe  qty models 3D\n",
    "#that save x models\n",
    "%matplotlib notebook\n",
    "\n",
    "results_qty_models = libraries.interpretability_methods.plotSenSpeQtyModelsByThreshold(vec_values_sen_spe_models)\n",
    "\n",
    "#display(results_qty_models)\n",
    "\n",
    "X = results_qty_models['sensitivity']\n",
    "Y = results_qty_models['specificity']\n",
    "Z = results_qty_models['qty_models']\n",
    "\n",
    "#y_axis_values = range(math.floor(min(Y)), math.ceil(max(Y))+1)\n",
    "#x_axis_values = range(math.floor(min(X)), math.ceil(max(X))+1)\n",
    "\n",
    "max_quantity = results_qty_models.loc[results_qty_models['qty_models'].idxmax()]\n",
    "max_quantity = int(max_quantity['qty_models'])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "surf = ax.plot_trisurf(X, Y, Z.values,  cmap=cm.YlGnBu, linewidth=0, antialiased=False)\n",
    "\n",
    "#ax.set_zlim(0, max_quantity)\n",
    "ax.set_zticks(Z)\n",
    "#ax.set_xticks(x_axis_values, minor=False)\n",
    "#ax.set_yticks(y_axis_values, minor=False)\n",
    "\n",
    "ax.set_xlabel('$Sensitivity$')\n",
    "ax.set_ylabel('$Specificity$')\n",
    "\n",
    "\n",
    "\n",
    "ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
    "\n",
    "\n",
    "\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "plt.title('Sen/Spe threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the base of this plot, you should decide on threshold values for both, specificity and sensitivity and apply them. The resulting subset of selected models is shown in the scatterplot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select values sen spe filtre\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "#Put a limit in sen/spe\n",
    "#Here you put the threshold for the sensitivity and specificity\n",
    "#Don't forget to shave the plot and comment into your repport\n",
    "#--------------------\n",
    "value_sensitivity = 0.6\n",
    "value_specificity = 0.6\n",
    "#--------------------\n",
    "\n",
    "\n",
    "#We apply them\n",
    "\n",
    "vec_values_sen_spe_models_filtered = libraries.interpretability_methods.filterDataframeBySenSpeLimit(value_sensitivity, value_specificity, vec_values_sen_spe_models)\n",
    "vec_values_sen_spe_models_filtered_invert = libraries.interpretability_methods.filterDataframeBySenSpeLimitContrary(value_sensitivity, value_specificity, vec_values_sen_spe_models)\n",
    "\n",
    "\n",
    "figure = libraries.interpretability_plots.plotDataFrameValuesFiltered(value_sensitivity, value_specificity,vec_values_sen_spe_models_filtered, vec_values_sen_spe_models_filtered_invert)\n",
    "\n",
    "\n",
    "print('You have {0} models'.format(len(vec_values_sen_spe_models_filtered)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color:red;color:white\">Question 12</b>: Explain your choice of the threshold values for the sensitivity and specificity. (Save both plots into your reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Save the plot on the repport</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second selection: frequency-based filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a second model-selection filter is applied based on the \"importance\" of the features. Such feature importance is represented in this context by their relative presence (i.e. their frequency) among the models. \n",
    "\n",
    "#### Frequency of the variables\n",
    "The figure below shows the frequency of the variables among all the remaining models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "  \n",
    "    \n",
    "#--------------------\n",
    "list_models_path_complete = []\n",
    "for index, row in vec_values_sen_spe_models_filtered.iterrows():\n",
    "    model_path_complete = \"experiences/all_models/\" + str(row['file_name'])\n",
    "    list_models_path_complete.append(model_path_complete)\n",
    "#--------------------\n",
    "    \n",
    "#Perform the counting\n",
    "list_models_vars = libraries.interpretability_methods.transformModelsToModelVarObj(list_models_path_complete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram before cut\n",
    "\n",
    "dict_values_resultant = libraries.interpretability_methods.countVarFreq(list_models_vars)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#indication of the number of models and variables\n",
    "qty_models = len(list_models_vars)\n",
    "qty_variables = len(dict_values_resultant)\n",
    "print(\"You have {0} models and {1} variables\".format(qty_models, qty_variables))\n",
    "\n",
    "#Plot the new histogram\n",
    "libraries.interpretability_plots.plotHistogramFreqVar(dict_values_resultant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing a frequency threshold\n",
    "Filtering features by frequency will result in a reduction of both the number of features and the number of models, as models with eliminated variables are also eliminated. \n",
    "\n",
    "The plot below represents the number of models and variables that should remain after the filter is applied in function of the frequency threshold. It helps you to decide on which threshold to use for the filter.\n",
    "\n",
    "(Note that the frequency of a feature is calculated as the number of <b>different models</b> where it appears irrespective of the number of rules containing it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "#Perform the counting\n",
    "list_models_vars = libraries.interpretability_methods.transformModelsToModelVarObj(list_models_path_complete)\n",
    "dict_values = libraries.interpretability_methods.countVarFreq(list_models_vars)\n",
    "\n",
    "\n",
    "#TEST zone\n",
    "matrix_results = libraries.interpretability_methods.createPlotQtyVarPerModelByMinimumFreq(dict_values,list_models_vars)\n",
    "#display(matrix_results)\n",
    "#End test zone\n",
    "\n",
    "\n",
    "\n",
    "ax = plt.figure().gca()\n",
    "\n",
    "matrix_results.plot(kind='line',x='min freq var',y='number of models',ax=ax)\n",
    "matrix_results.plot(kind='line',x='min freq var',y='quantity of variables', color='red', ax=ax)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#libraries.interpretability_plots.plotFreqVarPerFreqMinimum(matrix_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the plot above, select the minimum frequency (threshold) for the variables on your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color:red;color:white\">Question 13</b>: Explain your choice of the threshold. (Save both plots into your report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to indicate the name of the file where you want to save the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valide the frequence value\n",
    "\n",
    "#Create a copy of the list that contains the model_var objects\n",
    "list_models_vars_cpopy = list_models_vars.copy()\n",
    "#select the minimum frequenty\n",
    "#--------------------\n",
    "nb_min_var = 58\n",
    "#--------------------\n",
    "\n",
    "#Perform the frequence\n",
    "list_model_var_resultant = libraries.interpretability_methods.reduceQtyVars(nb_min_var, dict_values,list_models_vars_cpopy)\n",
    "dict_values_resultant = libraries.interpretability_methods.countVarFreq(list_model_var_resultant)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#indication of the number of models and variables\n",
    "qty_models = len(list_model_var_resultant)\n",
    "qty_variables = len(dict_values_resultant)\n",
    "print(\"You have {0} models and {1} variables\".format(qty_models, qty_variables))\n",
    "\n",
    "#Plot the new histogram\n",
    "libraries.interpretability_plots.plotHistogramFreqVar(dict_values_resultant)\n",
    "#Show the frequency table\n",
    "dict_Values_ordered = libraries.interpretability_methods.sort_reverse_dictionary_by_values(dict_values_resultant)\n",
    "datafram_var_freq = pd.DataFrame(list(dict_Values_ordered.items()),columns=['Variable name','Frequence'])\n",
    "display(datafram_var_freq)\n",
    "\n",
    "\n",
    "#Perform the list of the models\n",
    "#--------------------\n",
    "file_name = 'models_selected.csv'\n",
    "#--------------------\n",
    "list_models_names=[model_var.model_path for model_var in list_model_var_resultant]\n",
    "dataframe_names_files = pd.DataFrame(list_models_names)\n",
    "dataframe_names_files.to_csv(file_name, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carlos: Don't forget to save the plot resultant of your choice...\n",
    "<br>\n",
    "<b>The objective of the lab is to arrived at the end with 5-10 models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analysis of the selected models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have selected the best models, they are saved on the file \"models_selected.CSV\" (Or other file if you change the name...)\n",
    "You may then load these models and use them to compute their predictions for the observations in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TrefleFIS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a4ef85f4bed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Import from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#--------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrefleFIS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tff_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"experiences/all_models/experiences/rmse_v2/exps_lab_lfa_rmse_v2_rmse_conf_A_CV_1_rule_5_var_per_rule_5.ftt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#--------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# In the future, it could possible to call clf.predict_classes() directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TrefleFIS' is not defined"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "# Import from file\n",
    "#--------------------\n",
    "fis = TrefleFIS.from_tff_file(\"experiences/all_models/experiences/rmse_v2/exps_lab_lfa_rmse_v2_rmse_conf_A_CV_1_rule_5_var_per_rule_5.ftt\")\n",
    "#--------------------\n",
    "# In the future, it could possible to call clf.predict_classes() directly\n",
    "# see issue #1\n",
    "y_pred_test = fis.predict(X_test)\n",
    "\n",
    "results_list_predictions = np.squeeze(np.asarray(y_pred_test))\n",
    "\n",
    "\n",
    "#libraries.results_plot.plotCMByTreflePredictions(y_test, results_list_predictions)\n",
    "#Convert your results into binary values\n",
    "results = []\n",
    "for element in y_pred_test:\n",
    "    if element > 0.5:\n",
    "        results.append(1)\n",
    "    else:\n",
    "        results.append(0)\n",
    "\n",
    "from libraries.ConfusionMatrix import ConfusionMatrix\n",
    "cm = confusion_matrix(y_test, results)\n",
    "n_classes = len(np.unique(y))\n",
    "ConfusionMatrix.plot(cm, classes=range(n_classes), title=\"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is only an example of how to load models and test their performance in the test set. (Remember that the test set is the one who has not been used during the previous training/selectionn steps.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><span style=\"background-color:red;color:white\">Question 14</span></b>: Among the final models, select three of them as follows: the smallest one (in terms of rules and variables), the best one (in terms of performance), and one in the \"middle\" that you consider as being a good trade-off between size and performance. With them:\n",
    "<ul>\n",
    "    <li>Apply them to the test set and analyze the results you obtained</li>\n",
    "    <li>Analyze them in terms of size, rules, vars per rules and other characteristics that you think are relevant</li>\n",
    "    <li>As far as possible, analyze their rules and try to \"explain\" their predictions.\n",
    "</ul>\n",
    "<br>\n",
    "Tips: You can use plots to described your results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
